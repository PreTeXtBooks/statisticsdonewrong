<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch-conclusion" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Conclusion</title>

  <p>
    Beware false confidence. You may soon develop a smug sense of
    satisfaction that <em>your</em> work doesn't screw up like everyone
    else's. But I have not given you a thorough introduction to the
    mathematics of data analysis. There are many ways to foul up statistics
    beyond these simple conceptual errors.
  </p>

  <p>
    Errors will occur often, because somehow, few undergraduate science
    degrees or medical schools require courses in statistics and
    experimental design <mdash/> and some introductory statistics courses
    skip over issues of statistical power and multiple inference. This is
    seen as acceptable despite the paramount role of data and statistical
    analysis in the pursuit of modern science; we wouldn't accept doctors
    who have no experience with prescription medication, so why do we
    accept scientists with no training in statistics? Scientists need
    formal statistical training and advice. To quote:
  </p>

  <blockquote>
    <p>
      <q>To consult the statistician after an experiment is finished is
      often merely to ask him to conduct a post mortem examination. He can
      perhaps say what the experiment died of.</q>
    </p>
    <attribution>R. A. Fisher, popularizer of the <em>p</em> value</attribution>
  </blockquote>

  <p>
    Journals may choose to reject research with poor-quality statistical
    analyses, and new guidelines and protocols may eliminate some problems,
    but until we have scientists adequately trained in the principles of
    statistics, experimental design and data analysis will not be improved.
    The all-consuming quest for statistical significance will only continue.
  </p>

  <p>
    Change will not be easy. Rigorous statistical standards don't come
    free: if scientists start routinely performing statistical power
    computations, for example, they'll soon discover they need vastly
    larger sample sizes to reach solid conclusions. Clinical trials are not
    free, and more expensive research means fewer published trials. You
    might object that scientific progress will be slowed needlessly
    <mdash/> but isn't it worse to build our progress on a foundation of
    unsound results?
  </p>

  <p>
    To any science students: invest in a statistics course or two while
    you have the chance. To researchers: invest in training, a good book,
    and statistical advice. And please, the next time you hear someone say
    <q>The result was significant with <m>p \lt 0.05</m>, so there's only
    a 1 in 20 chance it's a fluke!</q>, please beat them over the head
    with a statistics textbook for me.
  </p>

  <p>
    <em>Disclaimer:</em> The advice in this guide cannot substitute for
    the advice of a trained statistical professional. If you think you're
    suffering from any serious statistical error, please consult a
    statistician immediately. I shall not have any liability from any
    injury to your dignity, statistical error or misconception suffered as
    a result of your use of this website.
  </p>

  <p>
    Use of this guide to justify rejecting the results of a scientific
    study without reviewing the evidence in any detail whatsoever is
    grounds for being slapped upside the head with a very large statistics
    textbook. This guide should help you find statistical errors, not allow
    you to selectively ignore science you don't like.
  </p>

</chapter>
